{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dependencies"
      ],
      "metadata": {
        "id": "UkeUVYvi3eFs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7w7a0CahsmO",
        "outputId": "9ef4d02b-07be-4a6c-c422-dc77e1c228e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE3nGxTMh-L1"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Log in with your Hugging Face token (get it from https://huggingface.co/settings/tokens)\n",
        "login(token=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpvqLPPAiBmL"
      },
      "outputs": [],
      "source": [
        "pip install -r requirement.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuIz1waziKbx"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pickle\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model"
      ],
      "metadata": {
        "id": "xnKdOPlN3kS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGKSDH9xiPTf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the LLaMA model and tokenizer\n",
        "llama_model_name = \"meta-llama/Llama-3.2-1B\"  # Replace with the correct model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(llama_model_name, device_map='auto')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set up the HuggingFace pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=500, temperature=0.5, do_sample = True)\n",
        "llm = HuggingFacePipeline(pipeline=pipe, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Script"
      ],
      "metadata": {
        "id": "5-VLFYSc3ZLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TNOH2ZiiSoy"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "# Define the prompt template for a mental health consultant chatbot\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=(\n",
        "        \"You are a compassionate and highly knowledgeable mental health consultant specializing in stress, anxiety, and depression management. Your goal is to provide empathetic, practical, and evidence-based advice to individuals seeking support. \\n\\n\"\n",
        "        \"When responding to the following question, ensure your answer is:\\n\"\n",
        "        \"- *Complete*: Fully address the user's concerns with sufficient detail, but do not exceed 500 tokens.\\n\"\n",
        "        \"- *Clear and concise*: Use straightforward language and avoid unnecessary elaboration.\\n\"\n",
        "        \"- *Precise*: Focus directly on the user's question, providing actionable advice and avoiding irrelevant information.\\n\"\n",
        "        \"Patient's question: {question}\\n\\n\"\n",
        "        \"Your response:\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# TODO: Edit the prompt template so that it provides with a standalone response only, and nothing else\n",
        "\n",
        "# Create the chain\n",
        "qa_chain = LLMChain(llm=llm, prompt=prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRB56Dc1iWij"
      },
      "outputs": [],
      "source": [
        "chat_history = []  # Initialize an empty list to store conversation history\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"Ask me anything: \")\n",
        "    if user_question.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Exiting the chat. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append the user's question to the chat history\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "    # Combine chat history into a single context string\n",
        "    conversation_context = \"\\n\".join(\n",
        "        f\"{entry['role'].capitalize()}: {entry['content']}\" for entry in chat_history\n",
        "    )\n",
        "\n",
        "    # Run the QA chain with the correct input key\n",
        "    response = qa_chain.invoke({\"question\": user_question, \"history\": conversation_context})\n",
        "    # print(\"-----------------\")\n",
        "\n",
        "    # print(response)\n",
        "    # print(response['text'])\n",
        "    response = response['text']\n",
        "\n",
        "    # print(\"=================\")\n",
        "\n",
        "    # extract_from = \"Patient's question: \"\n",
        "\n",
        "    # extracted_answer = response.split(extract_from)[1].strip() if extract_from in response else response\n",
        "\n",
        "    # # Append the bot's response to the chat history\n",
        "    # chat_history.append({\"role\": \"bot\", \"content\": response})\n",
        "\n",
        "    # print(f\"Answer: {extracted_answer}\")\n",
        "# Extract the \"Your response:\" content\n",
        "    extract_from = \"Your response:\"\n",
        "    extracted_response = response.split(extract_from)[-1].strip() if extract_from in response else response\n",
        "\n",
        "    # Append the bot's response to the chat history\n",
        "    chat_history.append({\"role\": \"bot\", \"content\": extracted_response})\n",
        "\n",
        "    # Display the extracted response\n",
        "    print(f\"Answer: {extracted_response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "npRtCT9L1QLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "# File paths\n",
        "csv_file_path = \"sampled_file.csv\"  # Replace with your CSV file path\n",
        "json_file_path = \"train.json\"  # Replace with your desired JSON file path\n",
        "\n",
        "# Initialize a list to store the data\n",
        "data_list = []\n",
        "\n",
        "# Read the CSV file\n",
        "with open(csv_file_path, \"r\", encoding=\"utf-8-sig\") as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file)\n",
        "\n",
        "    # Strip whitespace from column names\n",
        "    csv_reader.fieldnames = [name.strip() for name in csv_reader.fieldnames]\n",
        "\n",
        "    # Debug: Print column names\n",
        "    print(f\"Column names in CSV: {csv_reader.fieldnames}\")\n",
        "\n",
        "    # Convert each row into a dictionary and add to the list\n",
        "    for row in csv_reader:\n",
        "        data_list.append({\n",
        "            # \"Context\": row.get(\"Context\", \"\").strip(),\n",
        "            # \"Response\": row.get(\"Response\", \"\").strip(),\n",
        "            # \"LLM\": row.get(\"LLM\", \"\").strip()\n",
        "            \"question\": row.get(\"Context\", \"\").strip(),\n",
        "            \"ideal_response\": row.get(\"Response\", \"\").strip(),\n",
        "        })\n",
        "\n",
        "# Write the data to a JSON file\n",
        "with open(json_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(data_list, json_file, indent=4)\n",
        "\n",
        "print(f\"Dataset converted to JSON format and saved as '{json_file_path}'\")\n"
      ],
      "metadata": {
        "id": "bTJ-UoAQ1Ok0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2lCmG4TiaxN"
      },
      "outputs": [],
      "source": [
        "# Sample test dataset (questions and ideal responses)\n",
        "# test_data = [\n",
        "#     {\n",
        "#         \"question\": \"How can I reduce stress?\",\n",
        "#         \"ideal_response\": \"To reduce stress, you can try relaxation techniques such as deep breathing, meditation, and progressive muscle relaxation. It's also important to get regular exercise, maintain a healthy diet, and ensure you're getting enough sleep. Seeking support from a mental health professional can also help you manage stress.\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"question\": \"What are the symptoms of anxiety?\",\n",
        "#         \"ideal_response\": \"Common symptoms of anxiety include restlessness, rapid heartbeat, excessive worry, trouble concentrating, irritability, and physical symptoms like sweating or trembling. If you experience these symptoms regularly, it may be helpful to talk to a healthcare provider.\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"question\": \"How do I know if I am depressed?\",\n",
        "#         \"ideal_response\": \"Depression can manifest in various ways, including persistent feelings of sadness, lack of energy, changes in appetite or sleep patterns, and loss of interest in activities you once enjoyed. If you notice these symptoms for two weeks or more, it's a good idea to speak with a mental health professional.\"\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "test_data = data_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bert-score\n"
      ],
      "metadata": {
        "id": "Us9_Zm_o7lbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import bert_score\n",
        "\n",
        "# Load the pre-trained Sentence Transformer model\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def get_cosine_similarity(text1, text2):\n",
        "    # Encode both texts into embeddings\n",
        "    embeddings1 = embedder.encode([text1])\n",
        "    embeddings2 = embedder.encode([text2])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(embeddings1, embeddings2)[0][0]\n",
        "    return similarity\n",
        "\n",
        "# Function to calculate BERTScore\n",
        "def get_bertscore(text1, text2):\n",
        "    # Use BERTScore to calculate precision, recall, and F1\n",
        "    P, R, F1 = bert_score.score([text1], [text2], lang='en')\n",
        "    return P.item(), R.item(), F1.item()\n",
        "\n",
        "# Function to evaluate chatbot's performance on the test dataset\n",
        "def evaluate_with_test_data(test_data, qa_chain):\n",
        "    total_cosine_score = 0\n",
        "    total_f1_score = 0\n",
        "    num_questions = len(test_data)\n",
        "\n",
        "    for entry in test_data:\n",
        "        question = entry[\"question\"]\n",
        "        ideal_response = entry[\"ideal_response\"]\n",
        "\n",
        "        # Generate chatbot's response using the QA chain\n",
        "        conversation_context = f\"User: {question}\\nBot: \"\n",
        "        chatbot_response = qa_chain.invoke({\"question\": question, \"history\": conversation_context})\n",
        "\n",
        "        # Extract the actual response from the chatbot's output\n",
        "        extract_from = \"Patient's question: \"\n",
        "        extracted_answer = chatbot_response.split(extract_from)[1].strip() if extract_from in chatbot_response else chatbot_response\n",
        "\n",
        "        # Calculate cosine similarity between ideal response and chatbot's response\n",
        "        cosine_sim = get_cosine_similarity(ideal_response, extracted_answer)\n",
        "\n",
        "        # Calculate BERTScore between ideal response and chatbot's response\n",
        "        P, R, F1 = get_bertscore(ideal_response, extracted_answer)\n",
        "\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Chatbot Response: {extracted_answer}\")\n",
        "        print(f\"Ideal Response: {ideal_response}\")\n",
        "        print(\"====================================================\")\n",
        "        print(f\"Cosine Similarity: {cosine_sim:.2f}\")\n",
        "        print(f\"BERTScore Precision: {P:.2f}, Recall: {R:.2f}, F1: {F1:.2f}\\n\")\n",
        "        print(\"====================================================\")\n",
        "        total_cosine_score += cosine_sim\n",
        "        total_f1_score += F1\n",
        "\n",
        "    # Calculate average scores for all questions\n",
        "    avg_cosine_score = total_cosine_score / num_questions\n",
        "    avg_f1_score = total_f1_score / num_questions\n",
        "\n",
        "    print(f\"Average Cosine Similarity Score: {avg_cosine_score:.2f}\")\n",
        "    print(f\"Average BERTScore F1: {avg_f1_score:.2f}\")\n",
        "\n",
        "    return avg_cosine_score, avg_f1_score\n",
        "\n",
        "# Run the evaluation (You will need to define `test_data` and `qa_chain` in your environment)\n",
        "evaluate_with_test_data(test_data, qa_chain)\n"
      ],
      "metadata": {
        "id": "JPrKMRrp5s4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xwmb-ToihMN"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# # Load the pre-trained Sentence Transformer model\n",
        "# embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# # Function to calculate cosine similarity\n",
        "# def get_cosine_similarity(text1, text2):\n",
        "#     # Encode both texts into embeddings\n",
        "#     embeddings1 = embedder.encode([text1])\n",
        "#     embeddings2 = embedder.encode([text2])\n",
        "\n",
        "#     # Calculate cosine similarity\n",
        "#     similarity = cosine_similarity(embeddings1, embeddings2)[0][0]\n",
        "#     return similarity\n",
        "\n",
        "# # Function to evaluate chatbot's performance on the test dataset\n",
        "# def evaluate_with_test_data():\n",
        "#     total_score = 0\n",
        "#     num_questions = len(test_data)\n",
        "\n",
        "#     for entry in test_data:\n",
        "#         question = entry[\"question\"]\n",
        "#         ideal_response = entry[\"ideal_response\"]\n",
        "\n",
        "#         # Generate chatbot's response using the QA chain\n",
        "#         conversation_context = f\"User: {question}\\nBot: \"\n",
        "#         chatbot_response = qa_chain.run({\"question\": question, \"history\": conversation_context})\n",
        "\n",
        "#         # Extract the actual response from the chatbot's output\n",
        "#         extract_from = \"Patient's question: \"\n",
        "#         extracted_answer = chatbot_response.split(extract_from)[1].strip() if extract_from in chatbot_response else chatbot_response\n",
        "\n",
        "#         # Calculate cosine similarity between ideal response and chatbot's response\n",
        "#         similarity = get_cosine_similarity(ideal_response, extracted_answer)\n",
        "\n",
        "#         print(f\"Question: {question}\")\n",
        "#         print(f\"Chatbot Response: {extracted_answer}\")\n",
        "#         print(f\"Ideal Response: {ideal_response}\")\n",
        "#         print(f\"Cosine Similarity: {similarity:.2f}\\n\")\n",
        "\n",
        "#         total_score += similarity\n",
        "\n",
        "#     # Calculate average score for all questions\n",
        "#     average_score = total_score / num_questions\n",
        "#     print(f\"Average Similarity Score: {average_score:.2f}\")\n",
        "#     return average_score\n",
        "\n",
        "# # Run the evaluation\n",
        "# evaluate_with_test_data()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "UkeUVYvi3eFs",
        "xnKdOPlN3kS4",
        "5-VLFYSc3ZLb",
        "npRtCT9L1QLK"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}